{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Apply DOC-Forest recipe to single subject BCI data\n\n\nHere we use the resting state DOC-Forest [1] recipe to analyze BCI data.\nCompared to the original reference, 2 major modifications are done.\n\n1) For simplicity, we only compute 1 feature per marker, not 4\n2) For speed, we use 200 trees, not 2000.\n\nCompared to the common spatial patterns example form the MNE website,\nthe result is not particularly impressive. This is because a\nglobal statistic like the mean or the standard deviation are a good\nabstraction for severly brain injured patients but not for different\nconditions in a BCI experiment conducted with healthy participants.\n\nReferences\n----------\n.. [1] Engemann D.A.*, Raimondo F.*, King JR., Rohaut B., Louppe G.,\n       Faugeras F., Annen J., Cassol H., Gosseries O., Fernandez-Slezak D.,\n       Laureys S., Naccache L., Dehaene S. and Sitt J.D. (2018).\n       Robust EEG-based cross-site and cross-protocol classification of\n       states of consciousness. Brain. doi:10.1093/brain/awy251\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Denis A. Engemann <denis.engemann@gmail.com>\n#          Federico Raimondo <federaimondo@gmail.com>\n\nimport numpy as np\nimport mne\n\nfrom mne.io import concatenate_raws, read_raw_edf\nfrom mne.datasets import eegbci\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import cross_val_score, GroupShuffleSplit\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.pipeline import make_pipeline\n\nimport matplotlib.pyplot as plt\n\nfrom nice import Markers\nfrom nice.markers import (PowerSpectralDensity,\n                          KolmogorovComplexity,\n                          PermutationEntropy,\n                          SymbolicMutualInformation,\n                          PowerSpectralDensitySummary,\n                          PowerSpectralDensityEstimator)\n\n# avoid classification of evoked responses by using epochs that start 1s after\n# cue onset.\ntmin, tmax = 0, 2\nevent_id = dict(hands=2, feet=3)\nsubject = 1\nruns = [6, 10, 14]  # motor imagery: hands vs feet\n\nraw_fnames = eegbci.load_data(subject, runs)\nraw_files = [read_raw_edf(f, preload=True, stim_channel='auto') for f in\n             raw_fnames]\nraw = concatenate_raws(raw_files)\nraw.filter(1, 45)\n\nmne.set_eeg_reference(raw, copy=False)\n\n# strip channel names of \".\" characters\nraw.rename_channels(lambda x: x.strip('.'))\n\nevents = mne.find_events(raw, shortest_event=0, stim_channel='STI 014')\n\npicks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n                       exclude='bads')\n\n# Read epochs (train will be done only between 1 and 2s)\n# Testing will be done with a running classifier\nepochs = mne.Epochs(\n    raw, events, event_id, tmin, tmax, proj=True, picks=picks,\n    baseline=None, preload=True)\n\npsds_params = dict(n_fft=4096, n_overlap=100, n_jobs='auto', nperseg=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare markers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "backend = 'python'  # This gives maximum compatibility across platforms.\n# For improved speed, checkout the optimization options using C extensions.\n\n# We define one base estimator to avoid recomputation when looking up markers.\nbase_psd = PowerSpectralDensityEstimator(\n    psd_method='welch', tmin=None, tmax=None, fmin=1., fmax=45.,\n    psd_params=psds_params, comment='default')\n\n\n# Here are the resting-state compatible markers we considered in the paper.\n\nmarkers = Markers([\n    PowerSpectralDensity(estimator=base_psd, fmin=1., fmax=4.,\n                         normalize=False, comment='delta'),\n    PowerSpectralDensity(estimator=base_psd, fmin=1., fmax=4.,\n                         normalize=True, comment='deltan'),\n    PowerSpectralDensity(estimator=base_psd, fmin=4., fmax=8.,\n                         normalize=False, comment='theta'),\n    PowerSpectralDensity(estimator=base_psd, fmin=4., fmax=8.,\n                         normalize=True, comment='thetan'),\n    PowerSpectralDensity(estimator=base_psd, fmin=8., fmax=12.,\n                         normalize=False, comment='alpha'),\n    PowerSpectralDensity(estimator=base_psd, fmin=8., fmax=12.,\n                         normalize=True, comment='alphan'),\n    PowerSpectralDensity(estimator=base_psd, fmin=12., fmax=30.,\n                         normalize=False, comment='beta'),\n    PowerSpectralDensity(estimator=base_psd, fmin=12., fmax=30.,\n                         normalize=True, comment='betan'),\n    PowerSpectralDensity(estimator=base_psd, fmin=30., fmax=45.,\n                         normalize=False, comment='gamma'),\n    PowerSpectralDensity(estimator=base_psd, fmin=30., fmax=45.,\n                         normalize=True, comment='gamman'),\n    PowerSpectralDensity(estimator=base_psd, fmin=1., fmax=45.,\n                         normalize=False, comment='summary_se'),\n    PowerSpectralDensitySummary(estimator=base_psd, fmin=1., fmax=45.,\n                                percentile=.5, comment='summary_msf'),\n    PowerSpectralDensitySummary(estimator=base_psd, fmin=1., fmax=45.,\n                                percentile=.9, comment='summary_sef90'),\n    PowerSpectralDensitySummary(estimator=base_psd, fmin=1., fmax=45.,\n                                percentile=.95, comment='summary_sef95'),\n    PermutationEntropy(tmin=None, tmax=0.6, backend=backend),\n    # csd needs to be skipped\n    SymbolicMutualInformation(\n        tmin=None, tmax=0.6, method='weighted', backend=backend,\n        method_params={'nthreads': 'auto', 'bypass_csd': True},\n        comment='weighted'),\n\n    KolmogorovComplexity(tmin=None, tmax=0.6, backend=backend,\n                         method_params={'nthreads': 'auto'}),\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare reductions.\nKeep in mind that this is BCI, we have some localized effects.\nTherefore we will consider the standard deviation acros channels.\nContraty to the paper, this is a single subject analysis. We therefore do\nnot pefrorm a full reduction but only compute one statistic\nper marker and per epoch. In the paper, instead, we computed summaries over\nepochs and sensosrs, yielding one value per marker per EEG recoding.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs_fun = np.mean\nchannels_fun = np.std\nreduction_params = {\n    'PowerSpectralDensity': {\n        'reduction_func': [\n            {'axis': 'frequency', 'function': np.sum},\n            {'axis': 'epochs', 'function': epochs_fun},\n            {'axis': 'channels', 'function': channels_fun}]\n    },\n    'PowerSpectralDensitySummary': {\n        'reduction_func': [\n            {'axis': 'epochs', 'function': epochs_fun},\n            {'axis': 'channels', 'function': channels_fun}]\n    },\n    'SymbolicMutualInformation': {\n        'reduction_func': [\n            {'axis': 'epochs', 'function': epochs_fun},\n            {'axis': 'channels', 'function': channels_fun},\n            {'axis': 'channels_y', 'function': channels_fun}]\n    },\n    'PermutationEntropy': {\n        'reduction_func': [\n            {'axis': 'epochs', 'function': epochs_fun},\n            {'axis': 'channels', 'function': channels_fun}]\n    },\n    'KolmogorovComplexity': {\n        'reduction_func': [\n            {'axis': 'epochs', 'function': epochs_fun},\n            {'axis': 'channels', 'function': channels_fun}]\n    }\n}\n\nX = np.empty((len(epochs), len(markers)))\nfor ii in range(len(epochs)):\n    markers.fit(epochs[ii])\n    X[ii, :] = markers.reduce_to_scalar(marker_params=reduction_params)\n    # XXX hide this inside code\n    for marker in markers.values():\n        delattr(marker, 'data_')\n    delattr(base_psd, 'data_')\n\ny = epochs.events[:, 2] - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Original DOC-Forest recipe\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# NOTE: It was 2000 in the paper. Bnut we want to save time.\nn_estimators = 200\ndoc_forest = make_pipeline(\n    RobustScaler(),\n    ExtraTreesClassifier(\n        n_estimators=n_estimators, max_features=1, criterion='entropy',\n        max_depth=4, random_state=42, class_weight='balanced'))\n\ncv = GroupShuffleSplit(n_splits=50, train_size=0.8, test_size=0.2,\n                       random_state=42)\n\naucs = cross_val_score(\n    X=X, y=y, estimator=doc_forest,\n    scoring='roc_auc', cv=cv, groups=np.arange(len(epochs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect variable importances\nWe will use, for convenience, the in-sample fit.\nIn the paper we sometimes looked at the distributions across CV-folds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "doc_forest.fit(X, y)\nvariable_importance = doc_forest.steps[-1][-1].feature_importances_\nsorter = variable_importance.argsort()\n\n# shorten the names a bit.\nvar_names = list(markers.keys())\nvar_names = [var_names[ii].lstrip('nice/marker/') for ii in sorter]\n\n# let's plot it\nplt.figure(figsize=(8, 6))\nplt.scatter(\n    doc_forest.steps[-1][-1].feature_importances_[sorter],\n    np.arange(17))\nplt.yticks(np.arange(17), var_names)\nplt.subplots_adjust(left=.46)\nplt.title('AUC={:0.3f}'.format(np.mean(aucs)))\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}